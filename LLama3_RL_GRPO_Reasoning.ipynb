{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNjP1sESebNN8Xe6BNfhLo1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amanzoni1/fine_tuning/blob/main/LLama3_RL_GRPO_Reasoning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Qwen3 Reinforcement Learning & GRPO with Reasoning"
      ],
      "metadata": {
        "id": "MAPVWSl58rau"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vDb1LzaXtMuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Colab Install { display-mode: \"form\" }\n",
        "%%capture\n",
        "# Install Unsloth + vLLM (pinned versions)\n",
        "!pip install --no-deps unsloth vllm==0.8.5.post1\n",
        "\n",
        "# Core dependencies for LoRA, TRL, and bitsandbytes on Colab\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft \"trl==0.15.2\" triton cut_cross_entropy unsloth_zoo\n",
        "\n",
        "# Common NLP libraries\n",
        "!pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub transformers==4.51.3\n",
        "\n",
        "# Evaluation‐metric\n",
        "!pip install evaluate rouge_score bert_score\n",
        "\n",
        "# vLLM extra requirements (skip numpy/transformers/xformers to avoid conflicts)\n",
        "import requests, re\n",
        "reqs = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "filtered = re.sub(rb\"(transformers|numpy|xformers)[^\\n]*\\n\", b\"\", reqs)\n",
        "with open(\"vllm_requirements.txt\",\"wb\") as f:\n",
        "    f.write(filtered)\n",
        "!pip install -r vllm_requirements.txt"
      ],
      "metadata": {
        "id": "aEa1BimRq-h1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6cUhQ5GfrlS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re, gc, os, getpass\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import TextStreamer\n",
        "from trl import SFTTrainer, SFTConfig, GRPOConfig, GRPOTrainer\n",
        "from vllm import SamplingParams\n",
        "from evaluate import load as load_metric"
      ],
      "metadata": {
        "id": "_mu5B4pgiy1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lesq5BNQl5X8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt for the token\n",
        "hf_token = getpass.getpass('Enter your HF access token and press enter: ')\n",
        "\n",
        "# Set the environment variable\n",
        "os.environ['HF_TOKEN'] = hf_token\n",
        "\n",
        "print(\"HF_TOKEN environment variable has been set.\")"
      ],
      "metadata": {
        "id": "bPWpRt1ltSa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hq1y7IWgtXnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\"\n",
        "hub_model = \"AManzoni/llama-grpo-rl\""
      ],
      "metadata": {
        "id": "gKKr5Lxrl5UK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name            = model,\n",
        "    max_seq_length        = 512,\n",
        "    load_in_4bit          = True,\n",
        "    fast_inference        = True,\n",
        "    max_lora_rank         = 16,\n",
        ")"
      ],
      "metadata": {
        "id": "T7X4QRg7iyzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    random_state               = 111,\n",
        "    r                          = 16,\n",
        "    lora_alpha                 = 32,\n",
        "    bias                       = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    target_modules             = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                                  \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        ")"
      ],
      "metadata": {
        "id": "nqtBNHJoiywq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "16WgoTCuNHMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "royIQRhy9_yE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define special tokens and system prompt\n",
        "reasoning_start = \"<REASONING>\"\n",
        "reasoning_end   = \"</REASONING>\"\n",
        "solution_start  = \"<SOLUTION>\"\n",
        "solution_end    = \"</SOLUTION>\"\n",
        "\n",
        "system_prompt = (\n",
        "    \"You are given a problem.\\n\"\n",
        "    \"Think over it and describe your step‐by‐step reasoning.\\n\"\n",
        "    f\"Enclose reasoning between {reasoning_start} and {reasoning_end}.\\n\"\n",
        "    f\"Finally, give your answer between {solution_start} and {solution_end}\"\n",
        ")"
      ],
      "metadata": {
        "id": "sRbGp0uc9_vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7IL_F9C19_s3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build and assign chat_template to the tokenizer\n",
        "\n",
        "chat_template = (\n",
        "    # If the very first message is a SYSTEM role, print it + <eos>:\n",
        "    \"{% if messages[0]['role'] == 'system' %}\"\n",
        "      \"{{ messages[0]['content'] + eos_token }}\"\n",
        "      \"{% set rest = messages[1:] %}\"\n",
        "    \"{% else %}\"\n",
        "      # Otherwise, inject our system_prompt + <eos>:\n",
        "      \"{{ '{system_prompt}' + eos_token }}\"\n",
        "      \"{% set rest = messages %}\"\n",
        "    \"{% endif %}\"\n",
        "\n",
        "    # Now loop over the remaining messages (either user or assistant):\n",
        "    \"{% for m in rest %}\"\n",
        "      \"{% if m['role'] == 'user' %}\"\n",
        "        \"{{ m['content'] }}\"\n",
        "      \"{% else %}\"  # assistant\n",
        "        \"{{ m['content'] + eos_token }}\"\n",
        "      \"{% endif %}\"\n",
        "    \"{% endfor %}\"\n",
        "\n",
        "    # If we asked for “add_generation_prompt,” append <REASONING> to the end:\n",
        "    \"{% if add_generation_prompt %}\"\n",
        "      \"{{ '{reasoning_start}' }}\"\n",
        "    \"{% endif %}\"\n",
        ")\n",
        "\n",
        "chat_template = chat_template\\\n",
        "    .replace(\"'{system_prompt}'\",   f\"'{system_prompt}'\")\\\n",
        "    .replace(\"'{reasoning_start}'\", f\"'{reasoning_start}'\")\n",
        "\n",
        "tokenizer.chat_template = chat_template"
      ],
      "metadata": {
        "id": "g6jtD5rL9_qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick sanity check of the template\n",
        "example_messages = [\n",
        "    {\"role\": \"user\",\n",
        "     \"content\": \"Which country has the highest population density?\"},\n",
        "    {\"role\": \"assistant\",\n",
        "     \"content\": (\n",
        "         f\"{reasoning_start}\"\n",
        "         \"I know that country X is small in area but has a huge population, \"\n",
        "         \"so its people per square kilometer is extremely high.\"\n",
        "         f\"{reasoning_end}\"\n",
        "         f\"{solution_start}Monaco{solution_end}\"\n",
        "     )},\n",
        "    {\"role\": \"user\",\n",
        "     \"content\": \"Which planet is farthest from the Sun?\"},\n",
        "]\n",
        "\n",
        "\n",
        "print(\"Rendered example:\\n\")\n",
        "print(tokenizer.apply_chat_template(example_messages, tokenize=False, add_generation_prompt = True))"
      ],
      "metadata": {
        "id": "JJ2R7THg9_lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kKC5jykT9_im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rOlq1e1nESSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset(\"openai/gsm8k\", \"main\", split=\"train\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "BIECi59HESN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== Raw GSM8K columns ===\")\n",
        "print(dataset.column_names)\n",
        "print(\"\\n=== First raw example ===\")\n",
        "print(dataset[0])"
      ],
      "metadata": {
        "id": "7bHJeDwgMVeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "skPAc5FfMVcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xnr2ptGhFlhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define formatting + token-count function\n",
        "def format_and_count_gsm8k(example):\n",
        "    question = example[\"question\"].strip()\n",
        "    reasoning = example[\"answer\"].split(\"####\")[0].replace(\"\\n\", \" \").strip()\n",
        "    final_ans = example[\"answer\"].split(\"####\")[1].strip()\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\",    \"content\": system_prompt},\n",
        "        {\"role\": \"user\",      \"content\": question},\n",
        "        {\"role\": \"assistant\", \"content\": (\n",
        "            f\"{reasoning_start}{reasoning}{reasoning_end}\"\n",
        "            f\"{solution_start}{final_ans}{solution_end}\"\n",
        "        )}\n",
        "    ]\n",
        "\n",
        "    enc = tokenizer.apply_chat_template(messages, tokenize=True)\n",
        "    if isinstance(enc, dict):\n",
        "        token_len = len(enc[\"input_ids\"])\n",
        "    else:\n",
        "        token_len = len(enc)\n",
        "\n",
        "    text_str = tokenizer.apply_chat_template(messages, tokenize=False)\n",
        "\n",
        "    return {\n",
        "        \"token_len\": token_len,\n",
        "        \"text\": text_str,\n",
        "        \"Messages\": messages\n",
        "    }"
      ],
      "metadata": {
        "id": "ajg3yBX1MVZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply formatting to the dataset\n",
        "dataset = dataset.map(\n",
        "    format_and_count_gsm8k,\n",
        "    remove_columns=dataset.column_names,\n",
        ")"
      ],
      "metadata": {
        "id": "7hN6-656MVXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check: print a few \"text\" examples before filtering\n",
        "print(\"\\n=== Few formatted text examples (first 3) ===\")\n",
        "for i in range(3):\n",
        "    print(f\"\\n--- Example {i} ---\")\n",
        "    print(dataset[i][\"text\"])"
      ],
      "metadata": {
        "id": "wJYHD4szKS9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kpmFycTeGcPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Token-length statistics and dataset filtered for training\n",
        "lengths = np.array(dataset[\"token_len\"])\n",
        "print(\"\\nToken-length percentiles (50/90/99):\", np.percentile(lengths, [50, 90, 99]))\n",
        "\n",
        "threshold = 200\n",
        "sft_ds_filtered     = dataset.filter(lambda ex: ex[\"token_len\"] <= threshold)\n",
        "sft_ds_filtered     = sft_ds_filtered.select(range(100))\n",
        "sft_ds_filtered_out = dataset.filter(lambda ex: ex[\"token_len\"] >  threshold)\n",
        "\n",
        "print(f\"\\nRemaining for training (≤{threshold} tokens): {len(sft_ds_filtered)} / {len(dataset)}\")"
      ],
      "metadata": {
        "id": "DLVYdi6wNqsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P_Mu32uANqqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop extra columns so dataset contains only \"text\"\n",
        "sft_dataset = sft_ds_filtered.remove_columns([\"token_len\", \"Messages\"])\n",
        "print(\"\\n=== Final dataset ===\")\n",
        "print(sft_dataset)"
      ],
      "metadata": {
        "id": "gOQvVF-1Nqk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lbIw7oOdLElH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W-Mih7ovLEjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "sft_config = SFTConfig(\n",
        "    seed                        = 111,\n",
        "    do_train                    = True,\n",
        "    num_train_epochs            = 2,\n",
        "    per_device_train_batch_size = 2,\n",
        "    gradient_accumulation_steps = 2,\n",
        "    learning_rate               = 2e-4,\n",
        "    lr_scheduler_type           = \"linear\",\n",
        "    warmup_ratio                = 0.03,\n",
        "    weight_decay                = 0.01,\n",
        "    logging_strategy            = \"steps\",\n",
        "    logging_steps               = 5,\n",
        "    report_to                   = \"none\",\n",
        ")"
      ],
      "metadata": {
        "id": "2KFvJ4DPLEgr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model         = model,\n",
        "    args          = sft_config,\n",
        "    train_dataset = sft_dataset,\n",
        "    tokenizer     = tokenizer,\n",
        ")"
      ],
      "metadata": {
        "id": "foW3_IKjLEeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "4E3PAUfqUecy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pbEwWzurUeaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick one example’s first two “system + user” messages\n",
        "prompt_messages = sft_ds_filtered_out[0][\"Messages\"][:2]\n",
        "\n",
        "# Render into a single string and append <REASONING> for generation:\n",
        "text = tokenizer.apply_chat_template(\n",
        "    prompt_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,  # append the final <REASONING>\n",
        ")"
      ],
      "metadata": {
        "id": "YuJS7pqWZ5si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stream the model’s generations (CoT + solution)\n",
        "streamer = TextStreamer(tokenizer, skip_prompt=False)\n",
        "\n",
        "_ = model.generate(\n",
        "    **tokenizer(text, return_tensors=\"pt\").to(\"cuda\"),\n",
        "    temperature    = 0.0,\n",
        "    max_new_tokens = 512,\n",
        "    streamer       = streamer,\n",
        ")"
      ],
      "metadata": {
        "id": "N03BjxciUeYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zMptRwFAGNMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hstKkm1OUeLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "03QFIBWqUeIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del dataset, sft_ds_filtered, sft_ds_filtered_out, sft_dataset\n",
        "\n",
        "# Free up Python objects and empty GPU cache\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "w_I9VfbHUeF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VEOuIJMXstrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yroPvjKbsto0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the full XSum train split into memory\n",
        "dataset = load_dataset(\"EdinburghNLP/xsum\", split=\"train\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "pmcBj1L_UeBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keep only the first 400 documents whose raw token count ≤ 300\n",
        "DOC_CUTOFF = 300\n",
        "TARGET_EXAMPLES = 400\n",
        "\n",
        "selected = []\n",
        "for ex in dataset:\n",
        "    # ex[\"document\"] is the source text, ex[\"summary\"] is the gold summary.\n",
        "    doc_tokens = tokenizer(\n",
        "        ex[\"document\"],\n",
        "        truncation=False,  # we just want to measure length, not truncate\n",
        "    )[\"input_ids\"]\n",
        "    if len(doc_tokens) <= DOC_CUTOFF:\n",
        "        selected.append({\n",
        "            \"document\": ex[\"document\"],\n",
        "            \"summary\":  ex[\"summary\"]\n",
        "        })\n",
        "        if len(selected) >= TARGET_EXAMPLES:\n",
        "            break\n",
        "\n",
        "print(f\"✔ Collected {len(selected)} examples with doc‐tokens ≤ {DOC_CUTOFF}.\")"
      ],
      "metadata": {
        "id": "K3lgX2E21LLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a Hugging Face Dataset from that Python list\n",
        "dataset = Dataset.from_list(selected)\n",
        "dataset"
      ],
      "metadata": {
        "id": "D1aNiWU71LIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust the prompt\n",
        "system_prompt = (\n",
        "    \"You are given a text.\\n\"\n",
        "    \"Think carefully about its main points and organize your reasoning.\\n\"\n",
        "    f\"Enclose reasoning between {reasoning_start} and {reasoning_end}.\\n\"\n",
        "    f\"Finally, provide the final summary between {solution_start} and {solution_end}\"\n",
        ")"
      ],
      "metadata": {
        "id": "oK7wGZdZ_cz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn each document into a formatted “text” string\n",
        "def to_grpo_input(ex):\n",
        "    # Build the `<SYSTEM> + <USER>` prompt, then append \"<REASONING>\" so model knows to start thinking.\n",
        "    messages = [\n",
        "        {\"role\": \"system\",  \"content\": system_prompt},\n",
        "        {\"role\": \"user\",    \"content\": ex[\"document\"]}\n",
        "    ]\n",
        "    text_str = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True  # this injects \"<REASONING>\" at the end\n",
        "    )\n",
        "    full_ids = tokenizer(text_str, truncation=False)[\"input_ids\"]\n",
        "\n",
        "    return {\n",
        "        \"prompt\":         text_str,\n",
        "        \"gold_summary\": ex[\"summary\"],\n",
        "        \"full_len\": len(full_ids)\n",
        "    }\n",
        "\n",
        "dataset = dataset.map(to_grpo_input)\n"
      ],
      "metadata": {
        "id": "iNhrr__UvFZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check token‐length percentiles on the newly‐built “text” field\n",
        "full_lens = np.array(dataset[\"full_len\"])\n",
        "\n",
        "print(\"✔ Final “prompt” lengths 50/90/99 pct:\", np.percentile(full_lens, [50,90,99]))"
      ],
      "metadata": {
        "id": "e2BZbpUO4IOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove every column except the needed columns\n",
        "rl_dataset = dataset.remove_columns([c for c in dataset.column_names\n",
        "                                    if c not in (\"prompt\",\"gold_summary\")])\n",
        "\n",
        "rl_dataset"
      ],
      "metadata": {
        "id": "CHANR_arp0YH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity check\n",
        "print(\"Ready for GRPO\")\n",
        "\n",
        "print(\"columns now:\", rl_dataset.column_names)\n",
        "\n",
        "for i in range(3):\n",
        "    print(f\"\\n─ Example {i} ─\")\n",
        "    print(\"text :\", repr(rl_dataset[i][\"prompt\"]))\n",
        "    print(\"gold :\", repr(rl_dataset[i][\"gold_summary\"]))"
      ],
      "metadata": {
        "id": "aAW_mPmnp0Vi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g_kpsf1Lp0S6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vb5rvfegKLZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build an “</SOLUTION> + optional EOS/whitespace” pattern\n",
        "solution_end_regex = (\n",
        "    r\"</SOLUTION>\"\n",
        "  + r\"[\\s]*\"\n",
        "  + \"(?:\" + re.escape(tokenizer.eos_token) + \")?\"\n",
        ")\n",
        "\n",
        "# Build a single regex that matches “</REASONING> … <SOLUTION> … </SOLUTION>” at the end\n",
        "match_full_format = re.compile(\n",
        "    rf\"{reasoning_end}\"        # literally \"</REASONING>\"\n",
        "    r\".*?\"                     # any characters (DOTALL)\n",
        "    rf\"{solution_start}\"       # literally \"<SOLUTION>\"\n",
        "    r\"(.+?)\"                   # (capture group for content, but we don’t use it here)\n",
        "    rf\"{solution_end_regex}\"   # the literal \"</SOLUTION>\" + optional whitespace + optional EOS\n",
        "    r\"[\\s]*$\"                  # anchored at the end of the string\n",
        ",   flags = re.MULTILINE | re.DOTALL\n",
        ")"
      ],
      "metadata": {
        "id": "LPgX45S50x-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reward func for the format\n",
        "def reward_format(completions, **kwargs):\n",
        "    \"\"\"\n",
        "    Returns one float per generated completion:\n",
        "      • +3.0  if there is exactly one </REASONING>,\n",
        "               exactly one <SOLUTION>, exactly one </SOLUTION>,\n",
        "               AND that entire sequence “</REASONING>…<SOLUTION>…</SOLUTION>”\n",
        "               sits at the very end of the string.\n",
        "      • Otherwise:\n",
        "          +0.5 if exactly one </REASONING>,     else −0.5\n",
        "          +0.5 if exactly one <SOLUTION>,       else −0.5\n",
        "          +0.5 if exactly one </SOLUTION>,      else −0.5\n",
        "        (so partial tag matches range from −1.5 … +1.5)\n",
        "      • If none of the three tags appear (all counts == 0), override to −3.0.\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    for c in completions:\n",
        "        if isinstance(c, list) and isinstance(c[0], dict) and \"content\" in c[0]:\n",
        "            resp = c[0][\"content\"]\n",
        "        elif isinstance(c, dict) and \"content\" in c:\n",
        "            resp = c[\"content\"]\n",
        "        else:\n",
        "            resp = str(c)\n",
        "\n",
        "        # Count how many times each tag appears\n",
        "        cnt_rend  = resp.count(reasoning_end)\n",
        "        cnt_sst   = resp.count(solution_start)\n",
        "        cnt_send  = resp.count(solution_end)\n",
        "\n",
        "        # Case A: “Perfect” only if exactly one of each tag *and* the regex matches at the very end\n",
        "        if (cnt_rend == 1) and (cnt_sst == 1) and (cnt_send == 1):\n",
        "            # Now verify the strict end‐of‐string match\n",
        "            if match_full_format.search(resp):\n",
        "                scores.append(3.0)\n",
        "                continue\n",
        "\n",
        "        # If none of the three tags appear → heavy penalty\n",
        "        if (cnt_rend == 0) and (cnt_sst == 0) and (cnt_send == 0):\n",
        "            scores.append(-3.0)\n",
        "            continue\n",
        "\n",
        "        # Otherwise, award partial credit/penalty per tag:\n",
        "        score = 0.0\n",
        "        score +=  0.5 if (cnt_rend  == 1) else -0.5\n",
        "        score +=  0.5 if (cnt_sst   == 1) else -0.5\n",
        "        score +=  0.5 if (cnt_send  == 1) else -0.5\n",
        "        scores.append(score)\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "zAk_WrA7MXYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QLykzcs9MXVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = load_metric(\"rouge\")\n",
        "bertscore = load_metric(\"bertscore\")"
      ],
      "metadata": {
        "id": "AWTb6a2iYw1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Capture everything between <SOLUTION> and </SOLUTION>\n",
        "solution_regex = re.compile(\n",
        "    rf\"{re.escape(solution_start)}(.*?){re.escape(solution_end)}\",\n",
        "    flags = re.DOTALL | re.MULTILINE\n",
        ")"
      ],
      "metadata": {
        "id": "ulpDkNkeNE07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract text from completion\n",
        "def extract_solution_text(raw: str) -> str:\n",
        "    m = solution_regex.search(raw)\n",
        "    if m:\n",
        "        return m.group(1).strip() or raw.strip()\n",
        "    return raw.strip()"
      ],
      "metadata": {
        "id": "--E21o1eWnpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a soft length‐reward function\n",
        "def length_reward(generated: str, reference: str) -> float:\n",
        "    len_gen = len(generated.split())\n",
        "    len_ref = len(reference.split())\n",
        "    dev = abs(len_gen - len_ref) / max(1, len_ref)\n",
        "    return max(0.0, 1.0 - dev)"
      ],
      "metadata": {
        "id": "ioIUuco7Wnm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the reward_content function\n",
        "ROUGE_WEIGHT  = 9.0   # maximum points from ROUGE-2 + BERTScore\n",
        "LENGTH_WEIGHT = 1.0   # maximum points from length\n",
        "\n",
        "def reward_content(prompts, completions, gold_summary, **kwargs):\n",
        "    \"\"\"\n",
        "    For each generated completion, do:\n",
        "      a) Extract `gen_summary` between <SOLUTION>…</SOLUTION>, or raw text if no tags.\n",
        "      b) Compute:\n",
        "           - rouge2_f1  = ROUGE-2 F1(gen_summary, reference)\n",
        "           - bert_f1    = BERTScore F1(gen_summary, reference)\n",
        "         Then combined_similarity = (rouge2_f1 + bert_f1) / 2\n",
        "      c) Compute length_penalty = length_reward(gen_summary, reference)\n",
        "      d) final_score = (combined_similarity × ROUGE_WEIGHT)\n",
        "                     + (length_penalty      × LENGTH_WEIGHT)\n",
        "      → final_score ∈ [0, ROUGE_WEIGHT + LENGTH_WEIGHT].\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    for completion, reference in zip(completions, gold_summary):\n",
        "        if isinstance(completion, list) and isinstance(completion[0], dict) and \"content\" in completion[0]:\n",
        "            raw = completion[0][\"content\"]\n",
        "        elif isinstance(completion, dict) and \"content\" in completion:\n",
        "            raw = completion[\"content\"]\n",
        "        else:\n",
        "            raw = str(completion)\n",
        "        gen_summary = extract_solution_text(raw)\n",
        "\n",
        "        # ROUGE-2 F1 (lexical overlap)\n",
        "        r = rouge.compute(\n",
        "            predictions = [gen_summary],\n",
        "            references  = [reference],\n",
        "            rouge_types = [\"rouge2\"],\n",
        "            use_stemmer = True\n",
        "        )\n",
        "        rouge2_f1 = r[\"rouge2\"]\n",
        "\n",
        "        # BERTScore F1 (semantic similarity)\n",
        "        b = bertscore.compute(\n",
        "            predictions = [gen_summary],\n",
        "            references  = [reference],\n",
        "            model_type  = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "            num_layers  = 6\n",
        "        )\n",
        "        bert_f1 = b[\"f1\"][0]\n",
        "\n",
        "        # Length penalty\n",
        "        lp = length_reward(gen_summary, reference)\n",
        "\n",
        "        # Scale & sum\n",
        "        combined_sim = 0.5 * (rouge2_f1 + bert_f1)\n",
        "        final_score = (combined_sim * ROUGE_WEIGHT) + (lp * LENGTH_WEIGHT)\n",
        "        scores.append(final_score)\n",
        "\n",
        "    return scores"
      ],
      "metadata": {
        "id": "Syc3PrwxWnkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yooCDFhjWngU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IFacbtNZlwSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zis9RXLjlwQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vllm_sampling_params = SamplingParams(\n",
        "    seed                       = 111,\n",
        "    temperature                = 1.0,\n",
        "    min_p                      = 0.1,\n",
        "    top_p                      = 0.95,\n",
        "    top_k                      = 64,\n",
        "    stop                       = [tokenizer.eos_token],\n",
        "    include_stop_str_in_output = True,\n",
        ")"
      ],
      "metadata": {
        "id": "QntUdwrQlwNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grpo_config = GRPOConfig(\n",
        "    seed                        = 111,\n",
        "    use_vllm                    = True,\n",
        "    vllm_sampling_params        = vllm_sampling_params,\n",
        "    do_train                    = True,\n",
        "    num_train_epochs            = 2.0,\n",
        "    per_device_train_batch_size = 16,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    learning_rate               = 5e-6,\n",
        "    lr_scheduler_type           = \"linear\",\n",
        "    warmup_ratio                = 0.03,\n",
        "    weight_decay                = 0.01,\n",
        "    optim                       = \"adamw_8bit\",\n",
        "    num_generations             = 8,\n",
        "    max_completion_length       = 150,\n",
        "    logging_strategy            = \"steps\",\n",
        "    logging_steps               = 10,\n",
        "    report_to                   = \"none\",\n",
        "    output_dir                  = \"grpo_outputs\",\n",
        "    overwrite_output_dir        = True,\n",
        "    save_strategy               = \"epoch\",\n",
        "    push_to_hub                 = True,\n",
        "    hub_model_id                = hub_model,\n",
        ")"
      ],
      "metadata": {
        "id": "8jI8OYa-r92k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = GRPOTrainer(\n",
        "    model            = model,\n",
        "    processing_class = tokenizer,\n",
        "    train_dataset    = rl_dataset,\n",
        "    args             = grpo_config,\n",
        "    reward_funcs     = [reward_format,\n",
        "                        reward_content],\n",
        ")"
      ],
      "metadata": {
        "id": "XP73hPtDr90h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ],
      "metadata": {
        "id": "Vbj64Iw0wGo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-5sJXYyMwO6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "9abQt0Otr9yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qH7KOf95r9v9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final memory and time stats\n",
        "last_log = trainer.state.log_history[-1]\n",
        "\n",
        "# Extract runtime info\n",
        "train_seconds      = last_log[\"train_runtime\"]\n",
        "samples_per_second = last_log.get(\"train_samples_per_second\", None)\n",
        "\n",
        "# Recompute GPU memory stats\n",
        "used_memory     = round(torch.cuda.max_memory_reserved() / 1024**3, 2)\n",
        "used_for_lora   = round(used_memory - start_gpu_memory, 2)\n",
        "used_pct        = round(used_memory / max_memory * 100, 2)\n",
        "lora_pct        = round(used_for_lora / max_memory * 100, 2)\n",
        "\n",
        "# Print summary\n",
        "print(f\"Training time      : {train_seconds:.1f} seconds ({train_seconds/60:.2f} minutes)\")\n",
        "if samples_per_second:\n",
        "    print(f\"Throughput         : {samples_per_second:.1f} samples/second\")\n",
        "print(f\"Peak VRAM usage    : {used_memory} GB ({used_pct}% of max memory)\")\n",
        "print(f\"VRAM for training  : {used_for_lora} GB ({lora_pct}% of max memory)\")"
      ],
      "metadata": {
        "id": "shaGd6ayr9tR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_d66Hzqx_3bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lqjKvNQj_3TH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualize GRPO Training Metrics { display-mode: \"form\" }\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"pastel\")\n",
        "\n",
        "logs = trainer.state.log_history\n",
        "\n",
        "steps      = []\n",
        "losses     = []\n",
        "rewards    = []\n",
        "kl_penalty = []\n",
        "\n",
        "for log in logs:\n",
        "    if \"loss\" in log:\n",
        "        steps.append(log[\"step\"])\n",
        "        losses.append(log[\"loss\"])\n",
        "        rewards.append(log.get(\"reward\", None))\n",
        "        kl_penalty.append(log.get(\"kl\", None))\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Loss vs. Step\n",
        "plt.subplot(3, 1, 1)\n",
        "plt.plot(steps, losses, marker=\"o\", linestyle=\"-\", color=\"C0\")\n",
        "plt.ylabel(\"Policy Loss\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.title(\"Policy Loss\")\n",
        "\n",
        "# Reward vs. Step\n",
        "plt.subplot(3, 1, 2)\n",
        "plt.plot(steps, rewards, marker=\"s\", linestyle=\"--\", color=\"C1\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.title(\"Total Reward\")\n",
        "\n",
        "# KL Penalty vs. Step\n",
        "plt.subplot(3, 1, 3)\n",
        "plt.plot(steps, kl_penalty, marker=\"x\", linestyle=\"-.\", color=\"C2\")\n",
        "plt.xlabel(\"Training Step\")\n",
        "plt.ylabel(\"KL Penalty\")\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.title(\"KL Penalty\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9ppcJKhJRdvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4knGUKdYYqBE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}