{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amanzoni1/fine_tuning/blob/main/SFT_Gemma_7B_it.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N6P6sXjqXcI"
      },
      "source": [
        "# Fine-Tuning Gemma-7B-Instruct for Financial Sentiment Classification\n",
        "\n",
        "In this notebook we take **Gemma-7B-Instruct** and teach it to become an accurate financial sentiment analyzer.  We begin by loading and exploring the Financial PhraseBank headlines—using the Hugging Face **Datasets** library to dig into sentence lengths, vocabulary, and **Transformer** tokenization patterns.  Next, we transform each headline into a prompt→JSON completion pair, preparing our data for training.\n",
        "\n",
        "For the fine-tuning itself, we combine 4-bit quantization (via **bitsandbytes**) with LoRA adapters (via **PEFT** and **TRL**), letting us update only a small set of adapter weights while keeping VRAM usage in check through activation checkpointing.  During training we’ll periodically evaluate on a validation set to monitor generalization, and at the end we’ll save and push our final model to the Hugging Face Hub.  \n",
        "\n",
        "The goal is to have a memory-efficient, domain-specialized Gemma-7B that reliably classifies headlines as POSITIVE, NEUTRAL or NEGATIVE, and better understand every step of the training.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rv5vE6BXThFa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4DXEihTQ0_7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Irsy_2HUOwoO"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "# Prompt for the token\n",
        "hf_token = getpass.getpass('Enter your HF access token and press enter: ')\n",
        "\n",
        "# Set the environment variable\n",
        "os.environ['HF_TOKEN'] = hf_token\n",
        "\n",
        "print(\"HF_TOKEN environment variable has been set.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jyig5gYBOwl5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ua5QEb4fRZbx"
      },
      "outputs": [],
      "source": [
        "model_name = \"google/gemma-7b-it\"\n",
        "dataset_name = \"financial_phrasebank\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCOZTsNDXcQ7"
      },
      "source": [
        "# Load & Inspect Dataset\n",
        "After installing the libraries, we load the Financial PhraseBank “sentences_75agree” split and preview its schema, size, and a few example headlines to understand the label distribution. Then we compute and visualize basic text statistics (lengths, word counts), clean and plot the top tokens (overall and finance-specific), and chart sentiment counts to spot any imbalance.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ys13C9Yjp3i3"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet seaborn matplotlib\n",
        "!pip install -U --quiet torch transformers datasets accelerate bitsandbytes peft trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASURUH9fuUom"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from datasets import load_dataset, load_dataset_builder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9tTZRF8vO7F"
      },
      "outputs": [],
      "source": [
        "# Configure plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"pastel\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpqcAs8_6Qb_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNiYlhcVPIQl"
      },
      "outputs": [],
      "source": [
        "# Preview dataset metadata\n",
        "ds_builder = load_dataset_builder(dataset_name, \"sentences_75agree\", trust_remote_code=True)\n",
        "\n",
        "print(\"Description:\\n\", ds_builder.info.description[:500], \"...\\n\")\n",
        "print(\"Features:\", ds_builder.info.features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49shQj0ePIOE"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "raw = load_dataset(dataset_name, \"sentences_75agree\")\n",
        "\n",
        "print(\"\\nRaw dataset object:\\n\", raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRCk37KwPpXE"
      },
      "outputs": [],
      "source": [
        "# Extract the train split\n",
        "dataset = load_dataset(dataset_name, \"sentences_75agree\", split=\"train\")\n",
        "\n",
        "print(\"\\nDataset object:\\n\", dataset)\n",
        "print(f\"\\nUsing 'train' split with {len(dataset)} examples.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vT4r0R3XQSXD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjYBFbPUQDhn"
      },
      "source": [
        "### Sample Sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbGHfrSPK8w1"
      },
      "outputs": [],
      "source": [
        "# get the label names from the ClassLabel feature\n",
        "label_names = dataset.features[\"label\"].names\n",
        "\n",
        "for label_id, label_name in enumerate(label_names):\n",
        "    print(f\"\\n{label_name.upper()} examples:\")\n",
        "    # filter down to this label, then select the first 3\n",
        "    examples = (\n",
        "        dataset\n",
        "        .filter(lambda ex: ex[\"label\"] == label_id)\n",
        "        .select(range(3))\n",
        "    )\n",
        "    for i, ex in enumerate(examples, start=1):\n",
        "        print(f\"  {i}. {ex['sentence']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdhHc1vOQT_i"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccLBYeSDQUSy"
      },
      "source": [
        "### Sentiment Distribution\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMmnwtMzJqND"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Build label→count mapping\n",
        "label_names = dataset.features[\"label\"].names\n",
        "counts = Counter(label_names[l] for l in dataset[\"label\"])\n",
        "total = sum(counts.values())\n",
        "\n",
        "# Print counts and percentages\n",
        "for name, cnt in counts.items():\n",
        "    pct = cnt / total * 100\n",
        "    print(f\"{name.capitalize():9s}: {cnt} ({pct:.1f}%)\")\n",
        "\n",
        "# Plot count distribution\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "# Bar chart (Count)\n",
        "sns.barplot(x=list(counts.keys()), y=list(counts.values()), ax=ax1)\n",
        "ax1.set_title(\"Sentiment Distribution (Count)\")\n",
        "ax1.set_xlabel(\"Sentiment\")\n",
        "ax1.set_ylabel(\"Count\")\n",
        "\n",
        "# Pie chart (Percentage)\n",
        "ax2.pie(counts.values(), labels=counts.keys(), autopct=\"%1.1f%%\", startangle=90)\n",
        "ax2.set_title(\"Sentiment Distribution (Percentage)\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXg-zK8xJqKr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh1ltE116I4W"
      },
      "source": [
        "### Text Characteristics Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQRIdhXQJqIS"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "# Compute metrics\n",
        "df[\"char_count\"]     = df[\"sentence\"].str.len()\n",
        "df[\"word_count\"]     = df[\"sentence\"].str.split().str.len()\n",
        "df[\"avg_word_length\"]= df[\"sentence\"].apply(lambda s: np.mean([len(w) for w in s.split()]))\n",
        "\n",
        "# Print overall stats\n",
        "print(\"Overall statistics:\")\n",
        "print(f\" • Avg characters   : {df['char_count'].mean():.1f}\")\n",
        "print(f\" • Avg words        : {df['word_count'].mean():.1f}\")\n",
        "print(f\" • Avg word length  : {df['avg_word_length'].mean():.1f}\")\n",
        "\n",
        "# Distribution by sentiment\n",
        "print(\"\\nBy-sentiment averages:\")\n",
        "stats = df.groupby(df[\"label\"].map(lambda i: dataset.features[\"label\"].names[i])) \\\n",
        "           .agg({\n",
        "             \"char_count\":     [\"mean\",\"std\"],\n",
        "             \"word_count\":     [\"mean\",\"std\"],\n",
        "             \"avg_word_length\":[\"mean\",\"std\"]\n",
        "           }).round(1)\n",
        "print(stats)\n",
        "\n",
        "# Plotting\n",
        "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "sns.histplot(df, x=\"char_count\",   hue=df[\"label\"].map(lambda i: dataset.features[\"label\"].names[i]),\n",
        "             kde=False, ax=axes[0], bins=20).set(title=\"Char Count\")\n",
        "sns.histplot(df, x=\"word_count\",   hue=df[\"label\"].map(lambda i: dataset.features[\"label\"].names[i]),\n",
        "             kde=False, ax=axes[1], bins=15).set(title=\"Word Count\")\n",
        "sns.boxplot(x=df[\"label\"].map(lambda i: dataset.features[\"label\"].names[i]),\n",
        "            y=\"avg_word_length\", data=df, ax=axes[2]).set(title=\"Avg. Word Length\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ex-FdyyJJqF4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EE9_8D-HJqDf"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "\n",
        "# Build stopword set + punctuation\n",
        "stopwords = set(ENGLISH_STOP_WORDS) | set(string.punctuation)\n",
        "\n",
        "# Tokenize & filter out stopwords, tokens containing digits, and very short tokens\n",
        "tokens = []\n",
        "for sent in df[\"sentence\"].str.lower():\n",
        "    words = re.findall(r'\\b\\w+\\b', sent)\n",
        "    for w in words:\n",
        "        if (\n",
        "            w not in stopwords           # not a stopword/punctuation\n",
        "            and not re.search(r'\\d', w)  # no digits\n",
        "            and len(w) > 2               # longer than 2 chars\n",
        "        ):\n",
        "            tokens.append(w)\n",
        "\n",
        "# Get top 20 tokens now\n",
        "freq = Counter(tokens)\n",
        "top20 = freq.most_common(20)\n",
        "\n",
        "\n",
        "print(\"Cleaned vocabulary (no numbers, no stopwords)\")\n",
        "\n",
        "for word, count in top20:\n",
        "    print(f\"{word:12s}: {count}\")\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.barplot(x=[w for w,_ in top20], y=[c for _,c in top20])\n",
        "plt.title(\"Top 20 Tokens (Filtered)\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel(\"Token\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpKX9tD_6hhi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ydgCzwaJqBB"
      },
      "outputs": [],
      "source": [
        "# Define your terms and prepare a list of (term, sentiment, count)\n",
        "financial_terms = [\n",
        "    'revenue', 'profit', 'loss', 'earnings', 'growth', 'decline',\n",
        "    'stock', 'share', 'dividend', 'quarter', 'market', 'sales',\n",
        "    'analyst', 'forecast', 'billion', 'million', 'percent', 'business',\n",
        "    'beat', 'miss', 'guidance', 'outlook', 'performance', 'results'\n",
        "]\n",
        "\n",
        "records = []\n",
        "label_names = dataset.features[\"label\"].names\n",
        "\n",
        "for label_id, label_name in enumerate(label_names):\n",
        "    # extract all sentences for this sentiment\n",
        "    sub = df[df[\"label\"] == label_id][\"sentence\"].str.lower()\n",
        "    # count each term\n",
        "    term_counts = {term: sub.str.contains(term, regex=False).sum() for term in financial_terms}\n",
        "    for term, cnt in term_counts.items():\n",
        "        records.append({\"term\": term, \"sentiment\": label_name, \"count\": cnt})\n",
        "\n",
        "# Build a DataFrame and pick top N terms overall\n",
        "term_df = pd.DataFrame(records)\n",
        "overall = term_df.groupby(\"term\").sum().sort_values(\"count\", ascending=False).head(10).index.tolist()\n",
        "plot_df = term_df[term_df[\"term\"].isin(overall)]\n",
        "\n",
        "# Plot grouped bar chart\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.barplot(\n",
        "    data=plot_df,\n",
        "    x=\"term\", y=\"count\", hue=\"sentiment\",\n",
        "    order=overall\n",
        ")\n",
        "plt.title(\"Top 10 Financial Terms by Sentiment\", fontsize=14, fontweight='bold')\n",
        "plt.xlabel(\"Financial Term\")\n",
        "plt.ylabel(\"Count in Headlines\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.legend(title=\"Sentiment\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qs4OncyUIth9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNyyznIG2swQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y66oamLA9LYP"
      },
      "source": [
        "# Tokenizer\n",
        "We load Gemma’s tokenizer and inspect its vocabulary size, maximum sequence length, and special tokens, then tokenize all headlines (with truncation) to measure token counts and plot overall and per-sentiment distributions.  \n",
        "Next, we identify the top 20 subword tokens by frequency and display sample headline tokenizations to see how the tokenizer handles punctuation, numbers, and financial terms.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bm1Hclel2st0"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"right\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cXt4zzE2srS"
      },
      "outputs": [],
      "source": [
        "# Tokenizer Properties\n",
        "\n",
        "print(f\"Vocab size       : {tokenizer.vocab_size}\")\n",
        "print(f\"Max model length : {tokenizer.model_max_length}\")\n",
        "print(f\"Special tokens   : {tokenizer.all_special_tokens}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uk_dAYFz3uyJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbEiI4mEGgzz"
      },
      "source": [
        "### Token Length Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbNuymuE3uv0"
      },
      "outputs": [],
      "source": [
        "# Compute token counts for each headline\n",
        "token_lengths = [\n",
        "    len(tokenizer(sentence, truncation=True, max_length=256)[\"input_ids\"])\n",
        "    for sentence in df[\"sentence\"]\n",
        "]\n",
        "df[\"token_count\"] = token_lengths\n",
        "\n",
        "print(\"\\nOverall token count stats:\")\n",
        "print(df[\"token_count\"].describe().round(1))\n",
        "\n",
        "\n",
        "# Plot side-by-side distributions\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# a) All headlines\n",
        "ax1.hist(token_lengths, bins=20, alpha=0.7, edgecolor='black')\n",
        "ax1.set_title(\"Token Count Distribution (All)\")\n",
        "ax1.set_xlabel(\"Tokens per Headline\")\n",
        "ax1.set_ylabel(\"Frequency\")\n",
        "ax1.axvline(np.mean(token_lengths), color=\"red\", linestyle=\"--\", label=f\"Mean: {np.mean(token_lengths):.1f}\")\n",
        "ax1.legend()\n",
        "\n",
        "# b) By\n",
        "plot_order = [1, 2, 0]\n",
        "for label_id in plot_order:\n",
        "    lengths = df[df[\"label\"] == label_id][\"token_count\"]\n",
        "    ax2.hist(lengths, bins=15, alpha=0.6, label=label_names[label_id], edgecolor='black')\n",
        "\n",
        "ax2.set_title(\"Token Count by Sentiment\")\n",
        "ax2.set_xlabel(\"Tokens per Headline\")\n",
        "ax2.set_ylabel(\"Frequency\")\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3aKaYFR3utd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsyFfMVmGufm"
      },
      "source": [
        "### Tokenization Examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hniWI_3svcVC"
      },
      "outputs": [],
      "source": [
        "# Flatten all token IDs, then convert to tokens\n",
        "all_ids = [\n",
        "    tid\n",
        "    for sentence in df[\"sentence\"]\n",
        "    for tid in tokenizer(sentence)[\"input_ids\"]\n",
        "    if tid not in tokenizer.all_special_ids\n",
        "]\n",
        "all_tokens = [tokenizer.convert_ids_to_tokens(tid) for tid in all_ids]\n",
        "\n",
        "subword_counts = Counter(all_tokens).most_common(20)\n",
        "subwords, counts = zip(*subword_counts)\n",
        "\n",
        "print(\"Top 20 Subwords (Filtered)\")\n",
        "\n",
        "for tok, cnt in subword_counts:\n",
        "    print(f\"{tok:12s}: {cnt}\")\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.barplot(x=list(subwords), y=list(counts))\n",
        "plt.title(\"Top 20 Subwords by Frequency\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.xlabel(\"Subword Token\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO1NcBMrGrCT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMAGDpuLp3eR"
      },
      "outputs": [],
      "source": [
        "print(\"Sample Sentence Tokenizations\")\n",
        "\n",
        "sample_sentences = [\n",
        "    \"Apple reports record Q3 revenue growth of 15%\",\n",
        "    \"Tesla stock plunges after disappointing earnings\",\n",
        "    \"Fed raises rates by 0.25 bps amid inflation concerns\"\n",
        "]\n",
        "\n",
        "for sentence in sample_sentences:\n",
        "    toks = tokenizer.tokenize(sentence)\n",
        "    print(f\"\\nSentence: {sentence}\")\n",
        "    print(f\"Tokens ({len(toks)}): {toks}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBJQ_ATRp3b3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyDgA-MvFKGk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9HeLCEBBTve"
      },
      "source": [
        "# Formatting Data\n",
        "We implement a `format_example` function to convert each headline + label into a single prompt→JSON completion, tokenize it with padding/truncation, and generate attention and label masks so the model only trains on the completion.  \n",
        "Then we test this on a small sample—decoding both full sequences and masked labels—to confirm that only the sentiment field is supervised during fine-tuning.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQBdef2PMmF9"
      },
      "outputs": [],
      "source": [
        "# Mapping Function\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "label_names = dataset.features[\"label\"].names\n",
        "label_map = {i: name.upper() for i, name in enumerate(label_names)}\n",
        "\n",
        "def format_example(example):\n",
        "\n",
        "    # Map label →  sentiment\n",
        "    idx = example[\"label\"]\n",
        "    sentiment_label = label_map[idx]\n",
        "\n",
        "    # Build full text as one string\n",
        "    prompt = (\n",
        "        'Analyze the sentiment of this financial headline and respond with JSON format.\\n\\n'\n",
        "        f'Input: \"{example[\"sentence\"]}\"\\n'\n",
        "        'Output: {\"sentiment\": '\n",
        "    )\n",
        "    completion = f'\"{sentiment_label}\"}}'\n",
        "    full = prompt + completion\n",
        "\n",
        "    # Tokenize once, with truncation+padding\n",
        "    toks = tokenizer(\n",
        "        full,\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=None,\n",
        "        add_special_tokens=True\n",
        "    )\n",
        "    input_ids = toks[\"input_ids\"]\n",
        "    attention_mask = toks[\"attention_mask\"]\n",
        "\n",
        "    # Figure out where the completion starts\n",
        "    prompt_ids = tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "        padding=False,\n",
        "        add_special_tokens=True\n",
        "    )[\"input_ids\"]\n",
        "    prompt_len = len(prompt_ids)\n",
        "\n",
        "    # Build labels mask, handling edge cases\n",
        "    labels = [-100] * len(input_ids)  # Start with all ignored\n",
        "\n",
        "    # Only supervise tokens that are part of the completion AND not padding\n",
        "    for i in range(prompt_len, len(input_ids)):\n",
        "        if input_ids[i] != tokenizer.pad_token_id:\n",
        "            labels[i] = input_ids[i]\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JG-hxMftToDw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMixhtI8561B"
      },
      "source": [
        "### Formatting Check\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vu2v8A6XToA_"
      },
      "outputs": [],
      "source": [
        "# Tokenized length stats\n",
        "def analyze_tokenized_lengths(dataset_sample):\n",
        "    lengths = []\n",
        "    for example in dataset_sample:\n",
        "        out = format_example(example)\n",
        "        # count non-pad tokens\n",
        "        actual_len = sum(1 for tok in out[\"input_ids\"] if tok != tokenizer.pad_token_id)\n",
        "        lengths.append(actual_len)\n",
        "    print(f\"Sample size: {len(lengths)}\")\n",
        "    print(f\"Mean tokens (non-pad): {np.mean(lengths):.1f}\")\n",
        "    print(f\"Max tokens  (non-pad): {np.max(lengths)}\")\n",
        "    return lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cIlUxtcTn-b"
      },
      "outputs": [],
      "source": [
        "# Run on first 100 examples of your training data\n",
        "sample_ds = dataset.select(range(100))\n",
        "sample_lengths = analyze_tokenized_lengths(sample_ds)\n",
        "\n",
        "# Single-example formatting\n",
        "pos_idx = label_names.index(\"positive\")\n",
        "test_example = {\n",
        "    \"sentence\": \"Apple stock surges 15% after earnings beat\",\n",
        "    \"label\":    pos_idx\n",
        "}\n",
        "formatted = format_example(test_example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuGuKOowMmDo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2lsU5ZtXfbk"
      },
      "outputs": [],
      "source": [
        "def inspect_example(example):\n",
        "    out = format_example(example)\n",
        "    # Full prompt + completion\n",
        "    full = tokenizer.decode(out[\"input_ids\"], skip_special_tokens=False)\n",
        "    # Only the part we supervise\n",
        "    label_ids = [tok for tok, lab in zip(out[\"input_ids\"], out[\"labels\"]) if lab != -100]\n",
        "    supervised = tokenizer.decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    print(\"── Full Prompt+Completion ──\\n\", full, \"\\n\")\n",
        "    print(\"── Supervised Portion ──\\n\", supervised)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbdGRBdJMl-y"
      },
      "outputs": [],
      "source": [
        "# Debug a real dataset example\n",
        "print(\"\\n--- Debug Real Dataset Example ---\")\n",
        "inspect_example(dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ywdrmygOMl8Q"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNN-36PKYGvp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVWaYvIv6jow"
      },
      "source": [
        "# Split Dataset\n",
        "We split the original train split into 80/10/10 train/validation/test, then check each subset’s class distribution to ensure our hold-out sets are representative.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfancQvV59mm"
      },
      "outputs": [],
      "source": [
        "# Capture original columns so we can remove them after mapping\n",
        "orig_cols = dataset.column_names\n",
        "\n",
        "# Split into 80/10/10 train/val/test\n",
        "splits = dataset.train_test_split(test_size=0.20, seed=42)\n",
        "train_ds, temp_ds = splits[\"train\"], splits[\"test\"]\n",
        "\n",
        "val_test_splits = temp_ds.train_test_split(test_size=0.5, seed=42)\n",
        "val_ds, test_ds = val_test_splits[\"train\"], val_test_splits[\"test\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D4dsV8QX9qWw"
      },
      "outputs": [],
      "source": [
        "# Check split sizes\n",
        "print(f\"Train size: {len(train_ds)}\")\n",
        "print(f\"Val size: {len(val_ds)}\")\n",
        "print(f\"Test size: {len(test_ds)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uHLwuYs9T_R"
      },
      "outputs": [],
      "source": [
        "# Check class distribution in the datasets\n",
        "for split_name, split_ds in [\n",
        "    (\"Train\", train_ds),\n",
        "    (\"Validation\", val_ds),\n",
        "    (\"Test\", test_ds)\n",
        "]:\n",
        "    counts = Counter(split_ds[\"label\"])\n",
        "    total = sum(counts.values())\n",
        "\n",
        "    print(f\"\\n{split_name} class distribution:\")\n",
        "    for idx, cnt in counts.items():\n",
        "        name = label_names[idx]\n",
        "        print(f\"  {name:8s}: {cnt} ({cnt/total*100:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOCzLn3k9Uww"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p-59cpcieNak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRRbFU-V_eLk"
      },
      "source": [
        "# Mapping\n",
        "We apply `format_example` to every example in each split—dropping the old columns—so each record now contains `input_ids`, `attention_mask`, and `labels`, ready for the trainer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X68mJ1yb59kW"
      },
      "outputs": [],
      "source": [
        "# map with the formatting function\n",
        "train_ds = train_ds.map(format_example, remove_columns=orig_cols)\n",
        "val_ds = val_ds.map(format_example, remove_columns=orig_cols)\n",
        "test_ds = test_ds.map(format_example, remove_columns=orig_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjDeHwm759dR"
      },
      "outputs": [],
      "source": [
        "# Inspect a couple of formatted examples\n",
        "\n",
        "for i in [50, 51]:\n",
        "    print(f\"\\n--- Mapped Example ---\")\n",
        "    inp_ids = train_ds[i][\"input_ids\"]\n",
        "    lbls = train_ds[i][\"labels\"]\n",
        "\n",
        "    # full prompt+completion (with padding)\n",
        "    print(\"Full text:\\n\", tokenizer.decode(inp_ids, skip_special_tokens=False))\n",
        "\n",
        "    # supervised portion only\n",
        "    supervised = [tok for tok, lab in zip(inp_ids, lbls) if lab != -100]\n",
        "    print(\"Supervised part:\", tokenizer.decode(supervised, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92zbnkh9FQDr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdpH5mQAFP-2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7qSucs1_uWW"
      },
      "source": [
        "# Train\n",
        "Finally, we quantize Gemma-7B to 4-bit, inject LoRA adapters, configure our `SFTTrainer` with evaluation and checkpointing steps, and launch the fine-tuning loop to produce our domain-specialized sentiment classifier.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhEHV6woQuUU"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from trl import SFTTrainer, SFTConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-EQnL4rQuSI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1IPaL4QQuPw"
      },
      "outputs": [],
      "source": [
        "# Quantize in 4-bit\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit = True,\n",
        "    bnb_4bit_quant_type = \"nf4\",\n",
        "    bnb_4bit_use_double_quant = True,\n",
        "    bnb_4bit_compute_dtype = \"float16\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm4dQ0soQuNW"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map = \"auto\",\n",
        "    attn_implementation = \"eager\", # \"sdpa\"\n",
        "    quantization_config = bnb_config,\n",
        "    torch_dtype = \"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HLXbzh2SY5K"
      },
      "outputs": [],
      "source": [
        "# Inject LoRA adapters\n",
        "peft_cfg = LoraConfig(\n",
        "    r = 16,\n",
        "    lora_alpha = 32,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    task_type = \"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_cfg)\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model.enable_input_require_grads()\n",
        "model.config.use_cache = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Okp7jiOklpb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXo_ZJGvlKLy"
      },
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "sft_config = SFTConfig(\n",
        "    seed = 42,\n",
        "    num_train_epochs = 3.0,\n",
        "    per_device_train_batch_size = 8,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    learning_rate = 2e-4,\n",
        "    lr_scheduler_type = \"linear\",\n",
        "    warmup_ratio = 0.03,\n",
        "    weight_decay = 0.01,\n",
        "    dataloader_num_workers = 2,\n",
        "    dataloader_pin_memory = True,\n",
        "    fp16 = True,\n",
        "    do_eval = True,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps = 50,\n",
        "    per_device_eval_batch_size = 8,\n",
        "    logging_steps = 10,\n",
        "    report_to = \"none\",\n",
        "    output_dir = \"gemma-sentiment-qlora\",\n",
        "    overwrite_output_dir = True,\n",
        "    save_strategy = \"epoch\",\n",
        "    push_to_hub = True,\n",
        "    hub_model_id = \"AManzoni/gemma-sentiment-qlora\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHAXzBZRS52m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tvJPQPLiS50W"
      },
      "outputs": [],
      "source": [
        "# Instantiate SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    args = sft_config,\n",
        "    train_dataset = train_ds,\n",
        "    eval_dataset = val_ds,\n",
        "    processing_class = tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEHKZsJQSVC8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCc3wNQKSU6d"
      },
      "outputs": [],
      "source": [
        "# Memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXKQlziMcN1P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d6sO3YecQsy"
      },
      "outputs": [],
      "source": [
        "# attn_implementation=\"sdpa\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VLZjgo2S72T"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6MGzR3qUY2U"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1I4T8niUYqQ"
      },
      "outputs": [],
      "source": [
        "# Final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "\n",
        "print(f\"{trainer.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBOdy-XDUYnz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "67qlxatom2_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYnItmb4fRJr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCRLgUYNfRHP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5bgkY_TfRE5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRC1cIYVfRCX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q3QPIKz5fRAB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVgsrUhVfQ9e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_XzloaqfQ6m"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ijqa0z5MUYlT"
      },
      "outputs": [],
      "source": [
        "# test, inference, saving methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkTN3Ebz_n2f"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# 9) Built-in evaluation on validation set\n",
        "metrics = trainer.evaluate()\n",
        "print(\"\\n=== Validation Metrics ===\")\n",
        "print(f\"  * eval_loss      = {metrics['eval_loss']:.4f}\")\n",
        "print(f\"  * perplexity     = {math.exp(metrics['eval_loss']):.2f}\")\n",
        "\n",
        "# 10) Quick accuracy check by greedy-decoding the sentiment token\n",
        "def greedy_sentiment_accuracy(ds, n=200):\n",
        "    from tqdm.auto import tqdm\n",
        "\n",
        "    correct = 0\n",
        "    total   = 0\n",
        "    for ex in tqdm(ds.select(range(min(len(ds), n)))):\n",
        "        prompt = tokenizer.decode(ex[\"input_ids\"], skip_special_tokens=True)\n",
        "        # generate just the sentiment token + closing brace\n",
        "        out = trainer.model.generate(\n",
        "            ex[\"input_ids\"][None, :],\n",
        "            max_new_tokens=5,\n",
        "            num_beams=1,\n",
        "            do_sample=False,\n",
        "        )\n",
        "        gen = tokenizer.decode(out[0][ex[\"input_ids\"].index(tokenizer.eos_token_id)+1 :], skip_special_tokens=True)\n",
        "        # extract the first word inside quotes\n",
        "        pred = gen.strip().split('\"')[1]\n",
        "        true = label_names[ex[\"labels\"].index(next(l for l in ex[\"labels\"] if l!=-100))]\n",
        "        if pred.upper() == true.upper():\n",
        "            correct += 1\n",
        "        total += 1\n",
        "    return correct / total\n",
        "\n",
        "acc = greedy_sentiment_accuracy(val_ds, n=200)\n",
        "print(f\"\\nGreedy decoding accuracy on 200 val examples: {acc*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K89XfvsO_nzr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSYKGN98_nsW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMxWliwZ22ppRSjKJMarLIS",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}